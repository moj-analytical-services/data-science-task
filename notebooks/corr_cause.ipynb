{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "analysers.py\n",
    "\n",
    "A module providing object‑oriented classes for statistical analysis:\n",
    " - Correlation (Pearson, Spearman)\n",
    " - Mendelian Randomization (2‑stage least squares)\n",
    " - Causality tests (Conditional Mutual Information, Transfer Entropy, Granger Causality)\n",
    "\n",
    "Usage (from shell):\n",
    "    python src/analysers.py --help\n",
    "\n",
    "Or import in a notebook:\n",
    "    from src.analysers import CorrelationAnalyser, RandomizationAnalyser, CausalityAnalyser\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np                                  # Numerical operations\n",
    "import pandas as pd                                 # DataFrame handling\n",
    "from sklearn.preprocessing import KBinsDiscretizer   # Discretize continuous data\n",
    "from statsmodels.tsa.stattools import grangercausalitytests  # Granger causality tests\n",
    "\n",
    "\n",
    "class BaseAnalyser:\n",
    "    \"\"\"\n",
    "    Base class for all analysers.\n",
    "    Holds a pandas DataFrame and provides common functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the analyser with a DataFrame.\n",
    "\n",
    "        :param df: pandas DataFrame containing the data to analyse.\n",
    "        \"\"\"\n",
    "        self.df = df  # Store the DataFrame for later use\n",
    "\n",
    "\n",
    "class CorrelationAnalyser(BaseAnalyser):\n",
    "    \"\"\"\n",
    "    Analyser for computing correlations between two variables in the DataFrame.\n",
    "    Inherits from BaseAnalyser.\n",
    "    \"\"\"\n",
    "    def pearson(self, x: str, y: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute Pearson correlation coefficient between columns x and y.\n",
    "\n",
    "        :param x: name of the first numeric column\n",
    "        :param y: name of the second numeric column\n",
    "        :return: Pearson r (float)\n",
    "        \"\"\"\n",
    "        # Select the two columns and compute the correlation matrix,\n",
    "        # then extract the off-diagonal element at (0,1)\n",
    "        return self.df[[x, y]].corr(method='pearson').iloc[0, 1]\n",
    "\n",
    "    def spearman(self, x: str, y: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute Spearman rank correlation between columns x and y.\n",
    "\n",
    "        :param x: name of the first column\n",
    "        :param y: name of the second column\n",
    "        :return: Spearman rho (float)\n",
    "        \"\"\"\n",
    "        return self.df[[x, y]].corr(method='spearman').iloc[0, 1]\n",
    "\n",
    "\n",
    "class RandomizationAnalyser(BaseAnalyser):\n",
    "    \"\"\"\n",
    "    Analyser for Mendelian (instrumental-variable) randomization.\n",
    "    Implements a two-stage least squares procedure.\n",
    "    \"\"\"\n",
    "    def mendelian_randomization(self, exposure: str, outcome: str, instrument: str):\n",
    "        \"\"\"\n",
    "        Perform two-stage least squares:\n",
    "         1) Regress exposure on instrument\n",
    "         2) Regress outcome on predicted exposure from stage 1\n",
    "\n",
    "        :param exposure: name of the exposure column\n",
    "        :param outcome: name of the outcome column\n",
    "        :param instrument: name of the genetic instrument column\n",
    "        :return: statsmodels RegressionResults of stage‑2 regression\n",
    "        \"\"\"\n",
    "        import statsmodels.api as sm\n",
    "\n",
    "        # Drop rows with missing data in any of the three columns\n",
    "        data = self.df.dropna(subset=[exposure, outcome, instrument])\n",
    "\n",
    "        # Stage 1: fit exposure ~ instrument + intercept\n",
    "        inst = sm.add_constant(data[instrument])       # add constant term\n",
    "        model1 = sm.OLS(data[exposure], inst).fit()    # OLS regression\n",
    "        exp_hat = model1.predict(inst)                 # predicted exposure\n",
    "\n",
    "        # Stage 2: fit outcome ~ predicted exposure + intercept\n",
    "        inst2 = sm.add_constant(exp_hat)               \n",
    "        model2 = sm.OLS(data[outcome], inst2).fit()\n",
    "        return model2  # return fitted model object\n",
    "\n",
    "\n",
    "class CausalityAnalyser(BaseAnalyser):\n",
    "    \"\"\"\n",
    "    Analyser for various causality metrics:\n",
    "     - Conditional Mutual Information (CMI)\n",
    "     - Transfer Entropy (TE)\n",
    "     - Granger Causality (GC)\n",
    "    \"\"\"\n",
    "    def conditional_mutual_information(self, x: str, y: str, z: str, n_bins: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Estimate I(X; Y | Z) by discretizing X, Y, Z into bins.\n",
    "\n",
    "        :param x: name of variable X\n",
    "        :param y: name of variable Y\n",
    "        :param z: name of conditioning variable Z\n",
    "        :param n_bins: number of bins for discretization\n",
    "        :return: estimated conditional mutual information\n",
    "        \"\"\"\n",
    "        # Select and drop rows with missing values\n",
    "        data = self.df[[x, y, z]].dropna()\n",
    "\n",
    "        # Discretize each variable into integer bins [0..n_bins-1]\n",
    "        disc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "        Xd, Yd, Zd = disc.fit_transform(data).astype(int).T\n",
    "        n = len(Xd)\n",
    "\n",
    "        # Count joint and marginal frequencies\n",
    "        from collections import Counter\n",
    "        p_xyz = Counter(zip(Xd, Yd, Zd))\n",
    "        p_xz  = Counter(zip(Xd, Zd))\n",
    "        p_yz  = Counter(zip(Yd, Zd))\n",
    "        p_z   = Counter(Zd)\n",
    "\n",
    "        # Compute CMI sum_{x,y,z} p(x,y,z) * log( (p(x,y,z)*p(z)) / (p(x,z)*p(y,z)) )\n",
    "        cmi = 0.0\n",
    "        for (xi, yi, zi), count in p_xyz.items():\n",
    "            p_xyz_val = count / n\n",
    "            p_xz_val  = p_xz[(xi, zi)] / n\n",
    "            p_yz_val  = p_yz[(yi, zi)] / n\n",
    "            p_z_val   = p_z[zi] / n\n",
    "            cmi += p_xyz_val * np.log((p_xyz_val * p_z_val) / (p_xz_val * p_yz_val) + 1e-12)\n",
    "        return cmi\n",
    "\n",
    "    def transfer_entropy(self, source: str, target: str, lag: int = 1, n_bins: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Estimate Transfer Entropy TE(source→target) ≈ I(source_{t-lag}; target_t | target_{t-lag})\n",
    "\n",
    "        :param source: name of source time series\n",
    "        :param target: name of target time series\n",
    "        :param lag: lag order\n",
    "        :param n_bins: number of bins for discretization\n",
    "        :return: estimated transfer entropy\n",
    "        \"\"\"\n",
    "        # Prepare lagged variables\n",
    "        df = self.df[[source, target]].dropna()\n",
    "        df['target_lag'] = df[target].shift(lag)\n",
    "        df['source_lag'] = df[source].shift(lag)\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Compute conditional mutual information for TE\n",
    "        return self.conditional_mutual_information('source_lag', target, 'target_lag', n_bins=n_bins)\n",
    "\n",
    "    def granger_causality(self, source: str, target: str, maxlag: int = 1, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform Granger causality test: does `source` help predict `target`?\n",
    "\n",
    "        :param source: name of source series\n",
    "        :param target: name of target series\n",
    "        :param maxlag: maximum lag to test\n",
    "        :return: dictionary of test results per lag\n",
    "        \"\"\"\n",
    "        data = self.df[[target, source]].dropna()\n",
    "        # Format: array [[target, source], ...]\n",
    "        arr = data.values\n",
    "        results = grangercausalitytests(arr, maxlag=maxlag, verbose=False)\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example CLI: python src/analysers.py --input data.csv --mode pearson --x col1 --y col2\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Run statistical analysers on a CSV file\")\n",
    "    parser.add_argument(\"--input\", \"-i\", required=True,\n",
    "                        help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"--mode\", \"-m\", required=True,\n",
    "                        choices=[\"pearson\", \"spearman\", \"mr\", \"cmi\", \"te\", \"gc\"],\n",
    "                        help=\"Analysis mode\")\n",
    "    parser.add_argument(\"--x\", help=\"Column X (for correlation, CMI, TE, GC)\")\n",
    "    parser.add_argument(\"--y\", help=\"Column Y (for correlation, CMI, TE, GC)\")\n",
    "    parser.add_argument(\"--z\", help=\"Column Z (for CMI)\")\n",
    "    parser.add_argument(\"--instrument\", help=\"Instrument column (for MR)\")\n",
    "    parser.add_argument(\"--exposure\", help=\"Exposure column (for MR)\")\n",
    "    parser.add_argument(\"--outcome\", help=\"Outcome column (for MR)\")\n",
    "    parser.add_argument(\"--lag\", type=int, default=1, help=\"Lag for TE/GC\")\n",
    "    parser.add_argument(\"--bins\", type=int, default=10, help=\"Bins for discretization\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(args.input)\n",
    "    if args.mode in [\"pearson\", \"spearman\"]:\n",
    "        corr = CorrelationAnalyser(df)\n",
    "        func = corr.pearson if args.mode == \"pearson\" else corr.spearman\n",
    "        print(f\"{args.mode}({args.x}, {args.y}) =\", func(args.x, args.y))\n",
    "\n",
    "    elif args.mode == \"mr\":\n",
    "        rnd = RandomizationAnalyser(df)\n",
    "        model = rnd.mendelian_randomization(args.exposure, args.outcome, args.instrument)\n",
    "        print(model.summary())\n",
    "\n",
    "    elif args.mode == \"cmi\":\n",
    "        caus = CausalityAnalyser(df)\n",
    "        print(\"CMI:\", caus.conditional_mutual_information(args.x, args.y, args.z, n_bins=args.bins))\n",
    "\n",
    "    elif args.mode == \"te\":\n",
    "        caus = CausalityAnalyser(df)\n",
    "        print(\"TE:\", caus.transfer_entropy(args.x, args.y, lag=args.lag, n_bins=args.bins))\n",
    "\n",
    "    elif args.mode == \"gc\":\n",
    "        caus = CausalityAnalyser(df)\n",
    "        res = caus.granger_causality(args.x, args.y, maxlag=args.lag)\n",
    "        print(\"Granger Causality results:\", res)\n",
    "\n",
    "\n",
    "\n",
    "# :\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# corr = CorrelationAnalyser(df)\n",
    "# print(\"Pearson r:\", corr.pearson('X', 'Y'))\n",
    "# rnd = RandomizationAnalyser(df)\n",
    "# mr_model = rnd.mendelian_randomization('exposure', 'outcome', 'instrument')\n",
    "# print(mr_model.summary())\n",
    "# caus = CausalityAnalyser(df)\n",
    "# print(\"Conditional MI:\", caus.conditional_mutual_information('X','Y','Z'))\n",
    "# print(\"Transfer Entropy:\", caus.transfer_entropy('X','Y'))\n",
    "# print(\"Granger Causality:\", caus.granger_causality('X','Y', maxlag=3))\n",
    "\n",
    "from src.analysers import (\n",
    "    EDAAnalyser, FeatureEngineer, ModelTrainer,\n",
    "    CorrelationAnalyser, RandomizationAnalyser, CausalityAnalyser\n",
    ")\n",
    "\n",
    "# 1) EDA\n",
    "eda = EDAAnalyser(df_sample)\n",
    "print(eda.summary())\n",
    "print(eda.missing_summary())\n",
    "\n",
    "# 2) Features\n",
    "fe = FeatureEngineer(df_sample)\n",
    "X, y = fe.get_features_and_target('outcome')\n",
    "X = fe.one_hot_encode(X, ['instrument'])\n",
    "X = fe.scale_numeric(X, ['exposure'])\n",
    "X_train, X_test, y_train, y_test = fe.train_test_split(X, y)\n",
    "\n",
    "# 3) Modeling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "mt = ModelTrainer(RandomForestClassifier(random_state=42),\n",
    "                  param_grid={'n_estimators': [50,100], 'max_depth': [3,5]})\n",
    "print(\"CV scores:\", mt.cross_validate(X_train, y_train))\n",
    "grid = mt.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Test eval:\", mt.evaluate(X_test, y_test))\n",
    "\n",
    "# 4) Stats\n",
    "corr = CorrelationAnalyser(df_sample)\n",
    "print(\"Pearson X/Y:\", corr.pearson('X','Y'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
