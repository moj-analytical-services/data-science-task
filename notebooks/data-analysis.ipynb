{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Principal Data Science Task Template\n",
    "\n",
    "**Author:** Leila Yousefi   \n",
    "**Date:** 24/07/2025  ({{ today().strftime(\"%Y-%m-%d\") }}\n",
    "**Objective:** Briefly restate the problem.\n",
    "\n",
    "## 1. Installations & Imports\n",
    "\n",
    "## 2. Data pre-processing\n",
    "### 2.1. load csv file into a dataframe\n",
    "### 2.2. Summary statistics\n",
    "### 2.3 Data Quality Checks & Solutions\n",
    "#### 2.3.1 Validation\n",
    "#### 2.3.2 Completeness\n",
    "#### 2.3.3 Uniqueness\n",
    "\n",
    "## 3. Exploratory Data Analysis\n",
    "### 3.1 Univariate distributions\n",
    "### 3.2 Bivariate relationships\n",
    "\n",
    "## 4. Feature Engineering & Modelling\n",
    "### 4.1 Train/test split\n",
    "\n",
    "\n",
    "## 5. Evaluation & Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Installations & Imports: Adjust or add libraries as needed for the task.\n",
    "\n",
    "# suppress that specific package RuntimeWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*invalid value encountered in cast.*\"\n",
    ")\n",
    "\n",
    "# standard libs\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# data libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# viz libs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Working directory\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Notebooks are here:\", os.listdir())\n",
    "\n",
    "# set paths\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Data pre-processing: Point the filepaths to data/raw/ and load data.\n",
    "\n",
    "### 2.1. load csv file into a dataframe\n",
    "filename = 'pre2018_linked_inv_lpa_data.csv'\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, filename), low_memory=False)\n",
    "\n",
    "# Display the first few records\n",
    "df.head()\n",
    "\n",
    "### 2.2 Summary statistics & missing values\n",
    "df.info()\n",
    "df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 2.3 Data Quality Checks & Solutions:\n",
    "\n",
    "#### 2.3.1 Validation: **Correct format** \n",
    "\n",
    "#### 2.3.2 Completeness: **Decisions on missing data**  \n",
    "- Column dates → drop rows (where both dates are missing)\n",
    "- Column X → make derieved id to detect and delete duplicates \n",
    "- Column Y → impute median  \n",
    "\n",
    "#### 2.3.3 Uniqueness: **Decisions onduplicates** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.1 Validation: **Correct format** \n",
    "# Convert to correct format \n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True) # force an out-of-bounds date to NaT, in addition to forcing non-dates (or non-parseable dates) to NaT\n",
    "# parses dates with the day first, e.g. \"10/11/12\" is parsed as 2012-11-10, yearfirst=True is not strict, but will prefer to parse with year first.\n",
    "\n",
    "# Count number of missing records based on missing values in 'registrationdate' 'date_received_in_opg'\n",
    "n_reg_missing = df['registrationdate'].isna().sum()\n",
    "n_opg_missing = df['date_received_in_opg'].isna().sum()\n",
    "print(f\"Missing registrationdate: {n_reg_missing}\")\n",
    "print(f\"Missing date_received_in_opg: {n_opg_missing}\")\n",
    "\n",
    "# Derive and Define year_month for monthly grouping\n",
    "df['year_month'] = df['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "#df['year'] = df['date_received_in_opg'].dt.to_period('Y').dt.to_timestamp()\n",
    "df['year'] = df['date_received_in_opg'].dt.year\n",
    "df['month'] = df['date_received_in_opg'].dt.month\n",
    "df['day'] = df['date_received_in_opg'].dt.day\n",
    "\n",
    "# Compute delay_days with null assignment for invalid dates\n",
    "# If registrationdate is NaT or after receipt, delay_days = NaN\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "invalid_mask = df['registrationdate'].isna() | (df['registrationdate'] > df['date_received_in_opg'])\n",
    "df.loc[invalid_mask, 'delay_days'] = pd.NA\n",
    "\n",
    "# compute “delay in days” and then fill any missing delays with the mean delay for that calendar year \n",
    "# (falling back to the overall mean only if an entire year-group is empty):\n",
    "\n",
    "# Filter out invalid or negative delays\n",
    "# Keep rows where delay_days is non-negative, drop NaN\n",
    "df = df[df['delay_days'].notna() & (df['delay_days'] >= 0)].copy()\n",
    "\n",
    "# Count number of missing records based on missing values in 'delay_days' \n",
    "n_delays_missing = df['delay_days'].isna().sum()\n",
    "print(f\"Missing delays: {n_delays_missing}\")\n",
    "delays_missing_ids = df[df['delay_days'].isna()]['case_no']\n",
    "#print(\"delays_missing_ids: \", delays_missing_ids)\n",
    "\n",
    "df['delay_year'] = (\n",
    "    df['registrationdate'].dt.year\n",
    "    .fillna(df['date_received_in_opg'].dt.year)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Pick a “year” to group on. Use registration‐year if present, otherwise receipt‐year.\n",
    "\n",
    "# Impute missing delays with the mean for that year\n",
    "df['delay_days'] = (\n",
    "    df\n",
    "    .groupby('delay_year')['delay_days']\n",
    "    .transform(lambda s: s.fillna(s.mean()))\n",
    ")\n",
    "\n",
    "# If an entire year had only missing delays, fill those with the overall mean\n",
    "overall_mean = df['delay_days'].mean()\n",
    "df['delay_days'] = df['delay_days'].fillna(overall_mean)\n",
    "\n",
    "# Count number of missing records based on missing values in 'delay_days' \n",
    "n_delays_missing = df['delay_days'].isna().sum()\n",
    "print(f\"Missing delays: {n_delays_missing}\")\n",
    "\n",
    "imputed_delays_days = df[df['case_no'].isin(delays_missing_ids)]['delay_days']\n",
    "print(f\"imputed delays (per day): {imputed_delays_days}\")\n",
    "\n",
    "print(f\"imputed df: {df}\")\n",
    "\n",
    "# clean up (Optional) \n",
    "df.drop(columns=['delay_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.2 Completeness: **Decisions on missing data** \n",
    "# Missing Data Imputation: Drop rows missing key dates\n",
    "df = df[df['registrationdate'].notna() & df['date_received_in_opg'].notna()]\n",
    "\n",
    "#### 2.3.2 Uniqueness: **Decisions onduplicates:**  \n",
    "# Remove duplicates\n",
    "# Build hybrid unique ID and remove duplicate\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        return f\"{row['case_no']}_{row['date_received_in_opg'].strftime('%Y%m%d')}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id')\n",
    "\n",
    "# Display processed dataframe\n",
    "print(\"The first few records:\", df.head(5))\n",
    "print(\"The last few records:\", df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.4 Accuracy: **measures the correctness of the content of data** \n",
    "# Establish which attributes of the data are required and \n",
    "# design the logic used to test them based on the business requirement. \n",
    "# Consistency is part of Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Exploratory Data Analysis: Insert code cells for plots and summary statistics.\n",
    "\n",
    "\n",
    "# Define the target variables among the columns\n",
    "#df[\"target\"] = df['delay_days']\n",
    "# df[\"target\"] = df[\"concern_type\"]\n",
    "\n",
    "### 3.1 Univariate distributions\n",
    "fig, ax = plt.subplots()\n",
    "df[\"delay_days\"].value_counts().plot(kind=\"bar\", ax=ax)\n",
    "plt.title(\"Target distribution\")\n",
    "\n",
    "\n",
    "# ### 3.2 Bivariate relationships\n",
    "# plt.scatter(df[\"feature1\"], df[\"feature2\"])\n",
    "# plt.xlabel(\"feature1\")\n",
    "# plt.ylabel(\"feature2\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define periods and concern types\n",
    "PERIODS = {\n",
    "    'Pre-pandemic (2016–17)':   (2017, [2016, 2017]),\n",
    "    'Spike (2018–19)':          (2019, [2018, 2019]),\n",
    "    'Pandemic (2020–21)':       (2021, [2020, 2021]),\n",
    "    'Post-pandemic (2022–23)':  (2023, [2022, 2023]),\n",
    "}\n",
    "TYPES = ['Financial', 'Health and Welfare', 'Both']\n",
    "\n",
    "# Compute monthly max and min delay in months\n",
    "# For each month and concern type, the “worst-case” delay expressed in months:\n",
    "# So after this step, every row belonging to, say, “Financial” in May 2018 \n",
    "# will have the same number—the largest delay_days observed among all Financial cases received in May 2018.\n",
    "# Max monthly delay in months\n",
    "df['max_delay_months'] = df.groupby(['year_month', 'concern_type'])['delay_days']\\\n",
    "                           .transform('max') / 30.44 # takes each row in a group and \n",
    "                                                     # replaces its value with the maximum of that group\n",
    "                                                    # divide by 30.44 (the average length of a month in days) \n",
    "                                                    # to convert that maximum-day figure into “months of delay.”\n",
    "# Min monthly delay in months\n",
    "df['min_delay_months'] = df.groupby(['year_month', 'concern_type'])['delay_days']\\\n",
    "                           .transform('min') / 30.44\n",
    "\n",
    "# Build DataFrame for distributions\n",
    "records = []\n",
    "for period, (reg_end, rec_years) in PERIODS.items():\n",
    "    mask = (\n",
    "        #(df['registrationdate'].dt.year <= reg_end) &\n",
    "        df['date_received_in_opg'].dt.year.isin(rec_years) &\n",
    "        df['concern_type'].isin(TYPES)\n",
    "    )\n",
    "    subset = df.loc[mask, ['concern_type', 'delay_days', 'max_delay_months', 'min_delay_months', 'year_month']].copy()\n",
    "    subset['period'] = period\n",
    "    records.append(subset)\n",
    "dist_df = pd.concat(records, ignore_index=True)\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df.rename(columns={\n",
    "    \"delay_days\": \"target\"\n",
    "}, inplace=True)\n",
    "\n",
    "X = df.drop(\"target\", axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Feature Engineering & Modelling: Develop pipelines under the specified headings and record decisions in Markdown.\n",
    "\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "\n",
    "# PCA can’t handle missing values directly. We have two main options:\n",
    "# Impute the missing values before you scale → PCA\n",
    "# Drop any rows (or columns) containing NaNs\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Select numeric columns\n",
    "num_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "\n",
    "# 2. Impute missing values (here we use the median)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_num_imputed = imputer.fit_transform(X[num_cols])\n",
    "\n",
    "# 3. Scale\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num_imputed)\n",
    "\n",
    "# 4. PCA to 90% explained variance\n",
    "pca = PCA(n_components=0.90, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_num_scaled)\n",
    "\n",
    "print(f\"PCA reduced {len(num_cols)} → {pca.n_components_} components\")\n",
    "\n",
    "# Or as a single Pipeline\n",
    "# This is handy if you plan to stick it into a larger Pipeline for CV/reproducibility:\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pca_pipeline = Pipeline([\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"pca\", PCA(n_components=0.90, random_state=RANDOM_STATE)),\n",
    "# ])\n",
    "\n",
    "# # fit + transform in one go\n",
    "# X_pca = pca_pipeline.fit_transform(X[num_cols])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca,         # or X_reduced if you use SelectKBest, etc.\n",
    "    y,             # or y_clean if you dropped rows\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Update your pipelines to force dense output\n",
    "num_feats = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "cat_feats = X.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),  # <-- note sparse=False\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_feats),\n",
    "    (\"cat\", cat_pipeline, cat_feats),\n",
    "    # optional: sparse_threshold=0  to force dense even if some parts remain sparse\n",
    "], sparse_threshold=0)\n",
    "\n",
    "# Build your full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"pca\", PCA(n_components=0.90, random_state=RANDOM_STATE)),\n",
    "    (\"clf\", LogisticRegression(random_state=RANDOM_STATE)),\n",
    "])\n",
    "\n",
    "# Cross-validate on the original DataFrame X, Series y\n",
    "scores = cross_val_score(full_pipeline, X, y, cv=5, scoring=\"roc_auc\")\n",
    "print(\"CV AUC:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluation & Next Steps: Clearly report metrics, visualizations, and recommended follow‑up actions.\n",
    "\n",
    "### 5.1 Final test performance\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict_proba(X_test)[:,1]\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, y_pred))\n",
    "print(classification_report(y_test, rf.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 5.2 Insights & Recommendations\n",
    "- **Key finding 1:** …\n",
    "- **Key finding 2:** …\n",
    "- **Limitations:** data quality, potential biases\n",
    "- **Next steps:** hyper-parameter tuning, fairness audit, productionize pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
