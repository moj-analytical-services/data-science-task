{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Principal Data Science Task Template\n",
    "\n",
    "**Author:** Leila Yousefi   \n",
    "**Date:** 24/07/2025  ({{ today().strftime(\"%Y-%m-%d\") }}\n",
    "**Objective:** Briefly restate the problem.\n",
    "\n",
    "## 1. Installations & Imports\n",
    "\n",
    "## 2. Data pre-processing\n",
    "### 2.1. load csv file into a dataframe\n",
    "### 2.2. Summary statistics\n",
    "### 2.3 Data Quality Checks & Solutions\n",
    "#### 2.3.1 Validation\n",
    "#### 2.3.2 Completeness\n",
    "#### 2.3.3 Uniqueness\n",
    "\n",
    "## 3. Exploratory Data Analysis\n",
    "### 3.1 Univariate distributions\n",
    "### 3.2 Bivariate relationships\n",
    "\n",
    "## 4. Feature Engineering & Modelling\n",
    "### 4.1 Train/test split\n",
    "\n",
    "\n",
    "## 5. Evaluation & Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Installations & Imports: Adjust or add libraries as needed for the task.\n",
    "\n",
    "# suppress that specific package RuntimeWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*invalid value encountered in cast.*\"\n",
    ")\n",
    "\n",
    "# standard libs\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# data libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# viz libs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Working directory\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Notebooks are here:\", os.listdir())\n",
    "\n",
    "# set paths\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Data pre-processing: Point the filepaths to data/raw/ and load data.\n",
    "\n",
    "### 2.1. load csv file into a dataframe\n",
    "filename = 'pre2018_linked_inv_lpa_data.csv'\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, filename), low_memory=False)\n",
    "\n",
    "# Display the first few records\n",
    "df.head()\n",
    "\n",
    "### 2.2 Summary statistics & missing values\n",
    "df.info()\n",
    "df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 2.3 Data Quality Checks & Solutions:\n",
    "\n",
    "#### 2.3.1 Validation: **Correct format** \n",
    "\n",
    "#### 2.3.2 Completeness: **Decisions on missing data**  \n",
    "- Column dates → drop rows (where both dates are missing)\n",
    "- Column X → make derieved id to detect and delete duplicates \n",
    "- Column Y → impute median  \n",
    "\n",
    "#### 2.3.3 Uniqueness: **Decisions onduplicates** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.1 Validation: **Correct format** \n",
    "# Convert to correct format \n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True) # force an out-of-bounds date to NaT, in addition to forcing non-dates (or non-parseable dates) to NaT\n",
    "# parses dates with the day first, e.g. \"10/11/12\" is parsed as 2012-11-10, yearfirst=True is not strict, but will prefer to parse with year first.\n",
    "\n",
    "# Count number of missing records based on missing values in 'registrationdate' 'date_received_in_opg'\n",
    "n_reg_missing = df['registrationdate'].isna().sum()\n",
    "n_opg_missing = df['date_received_in_opg'].isna().sum()\n",
    "print(f\"Missing registrationdate: {n_reg_missing}\")\n",
    "print(f\"Missing date_received_in_opg: {n_opg_missing}\")\n",
    "\n",
    "# derive and Define year_month for monthly grouping\n",
    "df['year_month'] = df['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "#df['year'] = df['date_received_in_opg'].dt.to_period('Y').dt.to_timestamp()\n",
    "df['year'] = df['date_received_in_opg'].dt.year\n",
    "df['month'] = df['date_received_in_opg'].dt.month\n",
    "df['day'] = df['date_received_in_opg'].dt.day\n",
    "\n",
    "# Compute delay in days\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "# compute “delay in days” and then fill any missing delays with the mean delay for that calendar year \n",
    "# (falling back to the overall mean only if an entire year-group is empty):\n",
    "\n",
    "# Count number of missing records based on missing values in 'delay_days' \n",
    "n_delays_missing = df['delay_days'].isna().sum()\n",
    "print(f\"Missing delays: {n_delays_missing}\")\n",
    "delays_missing_ids = df[df['delay_days'].isna()]['case_no']\n",
    "#print(\"delays_missing_ids: \", delays_missing_ids)\n",
    "\n",
    "df['delay_year'] = (\n",
    "    df['registrationdate'].dt.year\n",
    "    .fillna(df['date_received_in_opg'].dt.year)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Pick a “year” to group on. Use registration‐year if present, otherwise receipt‐year.\n",
    "\n",
    "# Impute missing delays with the mean for that year\n",
    "df['delay_days'] = (\n",
    "    df\n",
    "    .groupby('delay_year')['delay_days']\n",
    "    .transform(lambda s: s.fillna(s.mean()))\n",
    ")\n",
    "\n",
    "# If an entire year had only missing delays, fill those with the overall mean\n",
    "overall_mean = df['delay_days'].mean()\n",
    "df['delay_days'] = df['delay_days'].fillna(overall_mean)\n",
    "\n",
    "# Count number of missing records based on missing values in 'delay_days' \n",
    "n_delays_missing = df['delay_days'].isna().sum()\n",
    "print(f\"Missing delays: {n_delays_missing}\")\n",
    "\n",
    "imputed_delays_days = df[df['case_no'].isin(delays_missing_ids)]['delay_days']\n",
    "print(f\"imputed delays (per day): {imputed_delays_days}\")\n",
    "\n",
    "print(f\"imputed df: {df}\")\n",
    "\n",
    "# clean up (Optional) \n",
    "df.drop(columns=['delay_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.2 Completeness: **Decisions on missing data** \n",
    "# Missing Data Imputation: Drop rows missing key dates\n",
    "df = df[df['registrationdate'].notna() & df['date_received_in_opg'].notna()]\n",
    "\n",
    "#### 2.3.2 Uniqueness: **Decisions onduplicates:**  \n",
    "# Remove duplicates\n",
    "# Build hybrid unique ID and remove duplicate\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        return f\"{row['case_no']}_{row['date_received_in_opg'].strftime('%Y%m%d')}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id')\n",
    "\n",
    "# Display processed dataframe\n",
    "print(\"The first few records:\", df.head(5))\n",
    "print(\"The last few records:\", df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.4 Accuracy: **measures the correctness of the content of data** \n",
    "# Establish which attributes of the data are required and \n",
    "# design the logic used to test them based on the business requirement. \n",
    "# Consistency is part of Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Exploratory Data Analysis: Insert code cells for plots and summary statistics.\n",
    "\n",
    "\n",
    "# Define the target variables among the columns\n",
    "df[\"target\"] = df['delay_days']\n",
    "# df[\"target\"] = df[\"concern_type\"]\n",
    "\n",
    "### 3.1 Univariate distributions\n",
    "fig, ax = plt.subplots()\n",
    "df[\"target\"].value_counts().plot(kind=\"bar\", ax=ax)\n",
    "plt.title(\"Target distribution\")\n",
    "\n",
    "\n",
    "# ### 3.2 Bivariate relationships\n",
    "# plt.scatter(df[\"feature1\"], df[\"feature2\"])\n",
    "# plt.xlabel(\"feature1\")\n",
    "# plt.ylabel(\"feature2\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Engineering & Modelling: Develop pipelines under the specified headings and record decisions in Markdown.\n",
    "### 4.1 Train/test split\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2 Preprocessing pipelines\n",
    "num_feats = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "cat_feats = X.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_feats),\n",
    "    (\"cat\", cat_pipeline, cat_feats),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.3 Baseline model: Logistic Regression\n",
    "baseline = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(random_state=RANDOM_STATE)),\n",
    "])\n",
    "scores = cross_val_score(baseline, X_train, y_train, cv=5, scoring=\"roc_auc\")\n",
    "print(\"Baseline AUC:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.4 Advanced model: Random Forest\n",
    "rf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "])\n",
    "scores_rf = cross_val_score(rf, X_train, y_train, cv=5, scoring=\"roc_auc\")\n",
    "print(\"RF AUC:\", scores_rf.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluation & Next Steps: Clearly report metrics, visualizations, and recommended follow‑up actions.\n",
    "\n",
    "### 5.1 Final test performance\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict_proba(X_test)[:,1]\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, y_pred))\n",
    "print(classification_report(y_test, rf.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 5.2 Insights & Recommendations\n",
    "- **Key finding 1:** …\n",
    "- **Key finding 2:** …\n",
    "- **Limitations:** data quality, potential biases\n",
    "- **Next steps:** hyper-parameter tuning, fairness audit, productionize pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
